{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "id": "E83lWQpLZQUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import torch\n",
        "import cv2\n",
        "from PIL import Image\n",
        "from torchvision import transforms, models\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "# Download model from Hugging Face\n",
        "repo_id = \"potguy/efficientnet_clahe_fracture_classification\"\n",
        "filename = \"efficientnet_clahe_hf.pth\"\n",
        "model_path = hf_hub_download(repo_id=repo_id, filename=filename)\n",
        "print(f\"âœ… Model downloaded to: {model_path}\")\n",
        "\n",
        "# Load model\n",
        "model = models.efficientnet_b0(weights=None)\n",
        "model.classifier[1] = torch.nn.Linear(model.classifier[1].in_features, 2)\n",
        "model.load_state_dict(torch.load(model_path, map_location=\"cpu\"))\n",
        "model.eval()\n",
        "\n",
        "# Define preprocessing\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "def apply_clahe(image):\n",
        "    \"\"\"Apply CLAHE to an input image\"\"\"\n",
        "    img = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)  # Convert to grayscale\n",
        "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
        "    img_clahe = clahe.apply(img)\n",
        "    img_rgb = cv2.cvtColor(img_clahe, cv2.COLOR_GRAY2RGB)  # Convert back to 3 channels\n",
        "    return img_rgb\n",
        "\n",
        "def predict(image):\n",
        "    \"\"\"Make a fracture prediction on an uploaded image\"\"\"\n",
        "    image = apply_clahe(image)\n",
        "    image = Image.fromarray(image)  # Convert to PIL format\n",
        "    image = transform(image).unsqueeze(0)  # Apply transformations\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(image)\n",
        "        _, predicted = torch.max(output, 1)\n",
        "\n",
        "    return \"Fractured\" if predicted.item() == 1 else \"Not Fractured\"\n",
        "\n",
        "interface = gr.Interface(\n",
        "    fn=predict,  # Your model function\n",
        "    inputs=gr.Image(type=\"numpy\"),  # Upload image\n",
        "    outputs=\"text\",  # Display prediction\n",
        "    title=\"ðŸ¦´ AI Fracture Detection\",  # Custom Title\n",
        "    description=\"Upload an X-ray image to check for fractures. This AI model uses EfficientNet with CLAHE preprocessing.\",  # Custom Description\n",
        "    theme=\"default\",  # Other themes: \"huggingface\", \"soft\", \"dark\"\n",
        "    allow_flagging=\"never\"  # Removes flagging button\n",
        ")\n",
        "\n",
        "with gr.Blocks(css=\"\"\"\n",
        "    #title { text-align: center; font-size: 24px; }\n",
        "    #desc { text-align: center; font-style: italic; }\n",
        "    #image-container { display: flex; justify-content: center; } /* Centers image */\n",
        "\"\"\") as interface:\n",
        "    gr.Markdown(\"## ðŸ¦´ AI Fracture Detection\", elem_id=\"title\")\n",
        "    gr.Markdown(\"*Upload an X-ray image to check for fractures.*\", elem_id=\"desc\")\n",
        "\n",
        "    with gr.Column(elem_id=\"image-container\"):  # Centers the whole section\n",
        "        image = gr.Image(type=\"numpy\", label=\"Upload X-ray Image\")\n",
        "        output = gr.Textbox(label=\"Prediction\")\n",
        "\n",
        "    btn = gr.Button(\"Analyze\")  # Button below output\n",
        "    btn.click(fn=predict, inputs=image, outputs=output)\n",
        "\n",
        "interface.launch(share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 705
        },
        "id": "LiCYs5-odO5w",
        "outputId": "0888fd8f-7fc4-42be-d9e9-2b511091cbad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Model downloaded to: /root/.cache/huggingface/hub/models--potguy--efficientnet_clahe_fracture_classification/snapshots/6b74521c73d092f53b83dda9f0bea659bd01d543/efficientnet_clahe_hf.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-a9ea2a736841>:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_path, map_location=\"cpu\"))\n",
            "/usr/local/lib/python3.11/dist-packages/gradio/interface.py:403: UserWarning: The `allow_flagging` parameter in `Interface` is deprecated.Use `flagging_mode` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://0a240a8e2417be618a.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://0a240a8e2417be618a.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mVjPjmTVda_Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}